{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import Augmentor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from tqdm import tqdm_notebook\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "'''Data Processing'''\n",
    "#importing train data and labels\n",
    "data_training = scipy.io.loadmat('Augmented_images.mat')\n",
    "TrainingPatches = data_training['TrainingPatches']\n",
    "TrainingGTLabels = data_training['TrainingGTLabels'][0]\n",
    "\n",
    "#import test data and labels\n",
    "data_testing = scipy.io.loadmat('FullTestData.mat')\n",
    "TestPatches = data_testing['TestPatches']\n",
    "TestGTLabels = data_testing['TestGTLabels'][0]\n",
    "\n",
    "# convert train data shape from [x,y, len] to  [len, 1, x, y]\n",
    "inputs, train_labels = TrainingPatches, TrainingGTLabels\n",
    "inputs = np.transpose(inputs, (2, 0, 1))\n",
    "inputs = np.expand_dims(inputs, 1)\n",
    "inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "\n",
    "# convert test data shape from [x,y, len] to  [len, 1, x, y]\n",
    "outputs, test_labels = TestPatches, TestGTLabels\n",
    "outputs = np.transpose(outputs, (2, 0, 1))\n",
    "outputs = np.expand_dims(outputs, 1)\n",
    "outputs = torch.tensor(outputs, dtype=torch.float32)\n",
    "\n",
    "inputs = inputs.view(inputs.size(0), 1, inputs.size(1), inputs.size(2), inputs.size(3))\n",
    "outputs = outputs.view(outputs.size(0), 1, outputs.size(1), outputs.size(2), outputs.size(3))\n",
    "\n",
    "'''Data Loader V1: small trial set of images'''\n",
    "\n",
    "class GLAUCOMA_small(torch.utils.data.Dataset): #include data in class\n",
    "   \n",
    "    def __init__(self, train_path, transform=None):\n",
    "\n",
    "        #importing train data and labels\n",
    "        data_training = scipy.io.loadmat(train_path)\n",
    "        TrainingPatches = data_training['TrainingPatches']\n",
    "        TrainingGTLabels = data_training['TrainingGTLabels'][0]\n",
    "        \n",
    "\n",
    "        #converting labels from 1,-1 to 1,0\n",
    "        test_labels = np.zeros((len(TrainingGTLabels), 2))\n",
    "\n",
    "        test_labels[:,0] = (TrainingGTLabels==-1)*1\n",
    "        test_labels[:,1] = (TrainingGTLabels==1)*1\n",
    "\n",
    "        data_training['TrainingGTLabels'] = test_labels\n",
    "        \n",
    "        TrainingGTLabels = data_training['TrainingGTLabels'][:,1]\n",
    "        \n",
    "        # convert train data shape from [30,30, 62022] to  [62022, 1, 30, 30]\n",
    "        inputs, train_labels = TrainingPatches, TrainingGTLabels\n",
    "        inputs = np.transpose(inputs, (2, 0, 1))\n",
    "        inputs = np.expand_dims(inputs, 1)\n",
    "        \n",
    "        # 900 training images / labels for equal no of class samples (4500 and 4500, 9000 just looks nice)\n",
    "        small_train_data = inputs#[0:9000]\n",
    "        small_train_labels = train_labels#[0:9000]\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "        self.train_data = small_train_data\n",
    "        self.train_labels = small_train_labels\n",
    "        print('shape of data:', np.shape(self.train_data))\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        # return right arguments if test, train supervised, train unsupervised\n",
    "        img, target = self.train_data[index], self.train_labels[index]\n",
    "        \n",
    "        #optional transform\n",
    "#         if self.transform != None:\n",
    "#         for trf in self.transform:\n",
    "#             if trf == 'rotations':\n",
    "#                 ones = small_train_data[small_train_labels == 1,:,:,:]\n",
    "#                 first = np.zeros(small_train_data.shape)\n",
    "#                 for i in range(small_train_data.shape[0]):\n",
    "#                     first[i,:,:,:] = np.rot90(ones)\n",
    "\n",
    "#         img = Image.fromarray(img.astype('uint8'))\n",
    "#         if self.transform is not None:\n",
    "#             img = self.transform(img)\n",
    "#             img = np.array(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_data)\n",
    "        \n",
    "temp = GLAUCOMA_small('Augmented_images.mat')\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(temp, batch_size=128, shuffle=True)\n",
    "\n",
    "temp = GLAUCOMA_small('FullTestData_NEW.mat')\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(temp, batch_size=128, shuffle=True)\n",
    "\n",
    "'''Define Initial Weights'''\n",
    "\n",
    "train_labels = scipy.io.loadmat('Augmented_images.mat')['TrainingGTLabels'].squeeze()\n",
    "w = float(sum(train_labels == 1))/sum(train_labels==-1)\n",
    "\n",
    "'''Define CNN Architecture'''\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 20, 3, 1)\n",
    "        # Max pooling over a (2, 2) window\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(20, 16, 2, 1)\n",
    "        self.conv3 = nn.Conv2d(16, 16, 2, 1)\n",
    "        #self.conv4 = nn.Conv2d(16, 16, 2, 1)\n",
    "        \n",
    "        # affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(576, 64)\n",
    "        self.fc2 = nn.Linear(64, 500)\n",
    "        self.fc3 = nn.Linear(500, 120)\n",
    "        self.fc4 = nn.Linear(120, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        print(x.shape)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        print(x.shape)\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        print(x.shape)\n",
    "        #x = self.pool(F.relu(self.conv4(x)))\n",
    "        print(x.shape)\n",
    "        \n",
    "        x = x.view(x.size(0),-1)\n",
    "        #x = x.view(-1, 16 * 6 * 6)\n",
    "        print(x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        print(x.shape)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        print(x.shape)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        print(x.shape)\n",
    "        #x = F.softmax(self.fc4(x))\n",
    "        x = F.softmax(F.relu(self.fc4(x)))\n",
    "        print(x.shape)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "net = Net()\n",
    "\n",
    "                                                                                                                        \n",
    "'''define loss function and optimiser'''\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.Tensor([w,1]))\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)      \n",
    "\n",
    "#train the network, feed the inputs to the network, optimise\n",
    "\n",
    "for epoch in range(2): #loop over the dataset\n",
    "     \n",
    "    running_loss = 0.0\n",
    "    all_outputs = []\n",
    "\n",
    "    for i, data in tqdm_notebook(enumerate(trainloader, 0)):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        inputs = inputs.type('torch.FloatTensor')\n",
    "        labels = labels.type('torch.LongTensor')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #forward + backward + optimise\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        all_outputs.append(outputs)\n",
    "        \n",
    "        #print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "'''Test Network'''\n",
    "\n",
    "#datacheck\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "print(images.size())\n",
    "print(labels.size())\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    y_pred = []\n",
    "    all_scores = []\n",
    "    first = True\n",
    "    for data in tqdm_notebook(testloader):\n",
    "        images, labels = data\n",
    "        images = images.type('torch.FloatTensor')\n",
    "        labels = labels.type('torch.LongTensor')())\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        scores = outputs.data[:,1].numpy()\n",
    "        y_pred.append(predicted.numpy())\n",
    "        all_scores.append(scores)\n",
    "        \n",
    "        if first:\n",
    "            first = False\n",
    "            y_actual = labels.numpy()\n",
    "        else:\n",
    "            y_actual = np.concatenate((y_actual, labels.numpy()))\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on test images: %d %%' % (\n",
    "    100 * correct / total))\n",
    "    \n",
    "'''Confusion Matrix and F score'''\n",
    "\n",
    "new_scores = all_scores[0]\n",
    "\n",
    "for i in range(1, len(all_scores)):\n",
    "    new_scores = np.concatenate((new_scores, all_scores[i]))\n",
    "    \n",
    "new_y_pred = y_pred[0]\n",
    "\n",
    "for i in range(1, len(y_pred)):\n",
    "    new_y_pred = np.concatenate((new_y_pred, y_pred[i]))\n",
    "    \n",
    "\n",
    "#ouputting confmat, precsion and recall\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score\n",
    "\n",
    "print(confusion_matrix(y_actual, new_y_pred))\n",
    "\n",
    "(recall_score(y_actual, new_y_pred), precision_score(y_actual, new_y_pred))\n",
    "\n",
    "#use scikit to find average precision-recall score and F1 score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "average_precision = average_precision_score(y_actual, new_scores)\n",
    "\n",
    "print('Average precision-recall score: %f' % average_precision)\n",
    "\n",
    "F1=2*((recall_score(y_actual, new_y_pred))*(precision_score(y_actual, new_y_pred)))/((recall_score(y_actual, new_y_pred))+(precision_score(y_actual, new_y_pred)))\n",
    "print('F1 score: %f' % F1)\n",
    "\n",
    "'''ROC curve'''\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_actual, new_scores)\n",
    "\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          average_precision))\n",
    "plt.savefig('C:/Users/mark/Desktop/PR_curve_augmented1500_2.png')\n",
    "plt.show()\n",
    "\n",
    "#to see how the two individual classes performed\n",
    "classes = ('pores', 'non-pores')\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images = images.type('torch.FloatTensor')\n",
    "        labels = labels.type('torch.LongTensor')\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "print(class_total)\n",
    "print(class_correct)\n",
    "#gives output for each class\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
